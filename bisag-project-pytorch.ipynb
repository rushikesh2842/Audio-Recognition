{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deepchecks -U --user","metadata":{"_uuid":"0dee2e57-37c5-45e8-aeca-10b30067c022","_cell_guid":"a61afd46-5981-4279-8bdf-db3628fefc3e","collapsed":false,"execution":{"iopub.status.busy":"2023-04-28T05:21:34.566287Z","iopub.execute_input":"2023-04-28T05:21:34.566709Z","iopub.status.idle":"2023-04-28T05:22:00.275361Z","shell.execute_reply.started":"2023-04-28T05:21:34.566680Z","shell.execute_reply":"2023-04-28T05:22:00.273923Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"_uuid":"f7c84c3a-3899-411d-906b-1b86e8e7253c","_cell_guid":"5b0f7617-e35c-4474-b5f6-13f2683e8fcb","collapsed":false,"execution":{"iopub.status.busy":"2023-04-28T05:22:00.281581Z","iopub.execute_input":"2023-04-28T05:22:00.282067Z","iopub.status.idle":"2023-04-28T05:22:10.497107Z","shell.execute_reply.started":"2023-04-28T05:22:00.282009Z","shell.execute_reply":"2023-04-28T05:22:10.495890Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio\nfrom torch import nn\nfrom torchsummary import summary\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"4b3e57d5-9626-44eb-b965-5e55429ad3ac","_cell_guid":"53c47148-5ef3-4bd6-b33b-a4e72c8066b1","collapsed":false,"execution":{"iopub.status.busy":"2023-04-28T05:22:10.500884Z","iopub.execute_input":"2023-04-28T05:22:10.501214Z","iopub.status.idle":"2023-04-28T05:22:13.187694Z","shell.execute_reply.started":"2023-04-28T05:22:10.501180Z","shell.execute_reply":"2023-04-28T05:22:13.186493Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n\n    def __init__(self,annotations_file,audio_dir,transformation,target_sample_rate,num_samples,device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device=device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal=signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]","metadata":{"_uuid":"58bdae65-09be-47fb-ba9b-93e70b479466","_cell_guid":"707152a8-cec1-4250-903b-37cc09782c06","collapsed":false,"execution":{"iopub.status.busy":"2023-04-28T05:22:13.190742Z","iopub.execute_input":"2023-04-28T05:22:13.191794Z","iopub.status.idle":"2023-04-28T05:22:13.205289Z","shell.execute_reply.started":"2023-04-28T05:22:13.191750Z","shell.execute_reply":"2023-04-28T05:22:13.204274Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNNNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 4 conv blocks / flatten / linear / softmax\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(128 * 5 * 4, 10)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions","metadata":{"_uuid":"6f0e541c-f5c3-4d5c-ba3b-c0f5a68743f9","_cell_guid":"feca65de-77d7-47c0-b9a8-0d6a2509bd2b","collapsed":false,"execution":{"iopub.status.busy":"2023-04-28T05:22:13.206853Z","iopub.execute_input":"2023-04-28T05:22:13.207753Z","iopub.status.idle":"2023-04-28T05:22:13.224364Z","shell.execute_reply.started":"2023-04-28T05:22:13.207712Z","shell.execute_reply":"2023-04-28T05:22:13.223255Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 10\nLEARNING_RATE = 0.001\nANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\nAUDIO_DIR = \"/kaggle/input/urbansound8k/\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050\n\ndef create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n    return train_dataloader\n\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in data_loader:\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")\n\n\ndef train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in tqdm(range(epochs)):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")\n\n\nif __name__ == \"__main__\":\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    \n    # instantiating our dataset object and create data loader\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    \n    train_dataloader = create_data_loader(usd, BATCH_SIZE)\n\n    # construct model and assign it to device\n    cnn = CNNNetwork().to(device)\n    print(cnn)\n\n    # initialise loss funtion + optimiser\n    loss_fn = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n\n    # train model\n    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n\n    # save model\n    torch.save(cnn.state_dict(), \"cnnnet.pth\")\n    print(\"Trained feed forward net saved at cnnnet.pth\")","metadata":{"_uuid":"03d30c84-d356-4ac2-85d9-e31bfe62464e","_cell_guid":"bfefb789-4883-48f5-b405-7068537ac14f","collapsed":false,"execution":{"iopub.status.busy":"2023-04-28T05:22:13.226104Z","iopub.execute_input":"2023-04-28T05:22:13.226456Z","iopub.status.idle":"2023-04-28T05:41:04.361512Z","shell.execute_reply.started":"2023-04-28T05:22:13.226420Z","shell.execute_reply":"2023-04-28T05:41:04.360372Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_mapping = [\n    \"air_conditioner\",\n    \"car_horn\",\n    \"children_playing\",\n    \"dog_bark\",\n    \"drilling\",\n    \"engine_idling\",\n    \"gun_shot\",\n    \"jackhammer\",\n    \"siren\",\n    \"street_music\"\n]\n\n\ndef predict(model, input, target, class_mapping):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(input)\n        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n        predicted_index = predictions[0].argmax(0)\n        predicted = class_mapping[predicted_index]\n        expected = class_mapping[target]\n    return predicted, expected\n\n\nif __name__ == \"__main__\":\n    # load back the model\n    cnn = CNNNetwork()\n    state_dict = torch.load(\"cnnnet.pth\")\n    cnn.load_state_dict(state_dict)\n\n    # load urban sound dataset dataset\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            \"cpu\")\n\n\n    # get a sample from the urban sound dataset for inference\n    input, target = usd[0][0], usd[0][1] # [batch size, num_channels, fr, time]\n    input.unsqueeze_(0)\n\n    # make an inference\n    predicted, expected = predict(cnn, input, target,\n                                  class_mapping)\n    print(f\"Predicted: '{predicted}', expected: '{expected}'\")","metadata":{"_uuid":"df9156fe-3225-47d5-be94-6ae74d5a73c4","_cell_guid":"a0af7088-6a25-4392-a64b-545851d4293f","collapsed":false,"execution":{"iopub.status.busy":"2023-04-28T05:41:04.363062Z","iopub.execute_input":"2023-04-28T05:41:04.363876Z","iopub.status.idle":"2023-04-28T05:41:04.550715Z","shell.execute_reply.started":"2023-04-28T05:41:04.363813Z","shell.execute_reply":"2023-04-28T05:41:04.549577Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"19e9d8df-ea38-4755-87f8-306bac66f797","_cell_guid":"aaf99631-4f79-4989-812d-5c63b41f34a0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}